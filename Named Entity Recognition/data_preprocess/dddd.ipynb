{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from poprogress import simple_progress as simp\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = pd.read_csv(\"train.csv\")\n",
    "# val_data = pd.read_csv(\"val.csv\")\n",
    "# test1_data = pd.read_csv(\"test1.csv\")\n",
    "# test2_data = pd.read_csv(\"test2.csv\")\n",
    "\n",
    "# train_len = len(train_data)\n",
    "# val_len = len(val_data)\n",
    "# test1_len = len(test1_data)\n",
    "# test2_len = len(test2_data)\n",
    "\n",
    "# print(\"train_len: \",train_len)\n",
    "# print(\"val_len: \",val_len)\n",
    "# print(\"test1_len: \",test1_len)\n",
    "# print(\"test2_len: \",test2_len)\n",
    "# train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_len:  21363\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_sentence</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EU rejects German call to boycott British lamb .</td>\n",
       "      <td>['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MIS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Peter Blackburn</td>\n",
       "      <td>['B-PER', 'I-PER']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BRUSSELS 1996-08-22</td>\n",
       "      <td>['B-LOC', 'O']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The European Commission said on Thursday it di...</td>\n",
       "      <td>['O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany 's representative to the European Unio...</td>\n",
       "      <td>['B-LOC', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        raw_sentence  \\\n",
       "0   EU rejects German call to boycott British lamb .   \n",
       "1                                    Peter Blackburn   \n",
       "2                                BRUSSELS 1996-08-22   \n",
       "3  The European Commission said on Thursday it di...   \n",
       "4  Germany 's representative to the European Unio...   \n",
       "\n",
       "                                              labels  \n",
       "0  ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MIS...  \n",
       "1                                 ['B-PER', 'I-PER']  \n",
       "2                                     ['B-LOC', 'O']  \n",
       "3  ['O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O...  \n",
       "4  ['B-LOC', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG'...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = pd.read_csv(\"all-data.csv\")\n",
    "all_len = len(all_data)\n",
    "print(\"all_len: \",all_len)\n",
    "all_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data, train_ratio, valid_ratio):\n",
    "\n",
    "    pool = np.random.rand(len(data)) \n",
    "    mask1 = pool < train_ratio\n",
    "    offset = train_ratio + valid_ratio\n",
    "    mask2 = (pool >= train_ratio) * (pool < offset)\n",
    "    train = data[mask1].reset_index(drop=True)\n",
    "    valid = data[mask2].reset_index(drop=True)\n",
    "    test = data[~(mask1 + mask2)].reset_index(drop=True)\n",
    "    \n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_size:  14941\n",
      "valid_data_size:  3238\n",
      "test_data_size:  3184\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, test_data = split_dataset(all_data, 0.7, 0.15)\n",
    "print(\"train_data_size: \",len(train_data))\n",
    "print(\"valid_data_size: \",len(valid_data))\n",
    "print(\"test_data_size: \",len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/14941 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14941/14941 [00:00<00:00, 63305.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-LOC': 0, 'B-MISC': 1, 'B-ORG': 2, 'B-PER': 3, 'I-LOC': 4, 'I-MISC': 5, 'I-ORG': 6, 'I-PER': 7, 'O': 8}\n",
      "{0: 'B-LOC', 1: 'B-MISC', 2: 'B-ORG', 3: 'B-PER', 4: 'I-LOC', 5: 'I-MISC', 6: 'I-ORG', 7: 'I-PER', 8: 'O'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_label_unique(data):\n",
    "    unique_label_list = []\n",
    "    for label in simp(data[\"labels\"]):\n",
    "        labels =  label.replace('[','').replace(']','').split(',')\n",
    "        for x in labels:\n",
    "            tag = x.replace(\"'\",'').replace(' ','')\n",
    "            if tag not in unique_label_list:\n",
    "                unique_label_list.append(tag)\n",
    "    return unique_label_list\n",
    "\n",
    "label_unique = sorted(get_label_unique(train_data))\n",
    "\n",
    "label_to_id = {k: v for v,k in enumerate(label_unique)}\n",
    "id_to_label = {k: v for k,v in enumerate(label_unique)}\n",
    "print(label_to_id)\n",
    "print(id_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.'],\n",
       " ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O'])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_tokens_labels(data, id):\n",
    "    \n",
    "    def get_sent_labels_list(data, id):\n",
    "        labels_list = []\n",
    "        label = data.loc[id, \"labels\"]\n",
    "        labels =  label.replace('[','').replace(']','').split(',')\n",
    "        for x in labels:\n",
    "            tag = x.replace(\"'\",'').replace(' ','')\n",
    "            labels_list.append(tag)\n",
    "        return labels_list\n",
    "    \n",
    "    def get_sent_tokens_list(data, id):\n",
    "        tokens_list = []\n",
    "        tokens = data.loc[id, \"raw_sentence\"].split()\n",
    "        for token in tokens:\n",
    "            tokens_list.append(token.lower())\n",
    "        return tokens_list\n",
    "\n",
    "    tokens_list = get_sent_tokens_list(data, id)\n",
    "    labels_list = get_sent_labels_list(data, id)\n",
    "    return tokens_list, labels_list\n",
    "\n",
    "get_tokens_labels(all_data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_seq(data):\n",
    "    data_token_seq, data_label_seq = [], []\n",
    "    for i in range(len(data)):\n",
    "        a, b = get_tokens_labels(data, i)\n",
    "        data_token_seq.append(a)\n",
    "        data_label_seq.append(b)\n",
    "    return data_token_seq, data_label_seq\n",
    "\n",
    "train_token_seq, train_label_seq = get_data_seq(train_data)\n",
    "\n",
    "token2cnt = Counter([token for sentence in train_token_seq for token in sentence])\n",
    "label_set = sorted(set(label for sentence in train_label_seq for label in sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peter\n",
      "blackburn\n",
      "B-PER\n",
      "I-PER\n"
     ]
    }
   ],
   "source": [
    "for x in train_token_seq:\n",
    "    for i in x:\n",
    "        print(i)\n",
    "    break\n",
    "\n",
    "for x in train_label_seq:\n",
    "    for i in x:\n",
    "        print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token2id(token2cnt, min_count = 1,add_pad = True, add_unk = True):\n",
    "    '''\n",
    "    Get mapping from tokens to indices to use with Embedding layer.\n",
    "    \n",
    "    param:\n",
    "        - min_count : Do not mark number if number of words less then this value.\n",
    "    '''\n",
    "    token_to_id = {}\n",
    "\n",
    "    if add_pad:\n",
    "        token_to_id[\"<PAD>\"] = len(token_to_id)\n",
    "    if add_unk:\n",
    "        token_to_id[\"<UNK>\"] = len(token_to_id)\n",
    "\n",
    "    for token, cnt in token2cnt.items():\n",
    "        if cnt >= min_count:\n",
    "            token_to_id[token] = len(token_to_id)\n",
    "\n",
    "    return token_to_id\n",
    "token_to_id = get_token2id(token2cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nerDataset\n",
    "def process_tokens(tokens, token2id, unk: str = \"<UNK>\"):\n",
    "    return [token2id.get(token, token2id[unk]) for token in tokens]\n",
    "\n",
    "def process_labels(labels,label2id):\n",
    "    return [label2id[label] for label in labels]\n",
    "\n",
    "class nerDataset(Dataset):\n",
    "\n",
    "    def __init__(self, token_seq, label_seq, token2id, label2id, preprocess:bool = True):\n",
    "        self.token2id = token2id\n",
    "        self.label2id = label2id\n",
    "        self.preprocess = preprocess\n",
    "        \n",
    "        if preprocess:\n",
    "            self.token_seq = [process_tokens(tokens, token2id) for tokens in token_seq]\n",
    "            self.label_seq = [process_labels(labels, label2id) for labels in label_seq]\n",
    "        else:\n",
    "            self.token_seq = token_seq \n",
    "            self.label_seq = label_seq  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_seq)\n",
    "\n",
    "    def __getitem__(self, id):\n",
    "        if self.preprocess:\n",
    "            tokens = self.token_seq[id]\n",
    "            labels = self.label_seq[id]\n",
    "        else:\n",
    "            tokens = process_tokens(self.token_seq[id], self.token2id) \n",
    "            labels = process_labels(self.label_seq[id], self.label2id) \n",
    "\n",
    "        lengths = [len(tokens)]\n",
    "\n",
    "        return np.array(tokens), np.array(labels), np.array(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = nerDataset(train_token_seq, train_label_seq, token_to_id, label_to_id, preprocess=True)\n",
    "\n",
    "valid_token_seq, valid_label_seq = get_data_seq(valid_data)\n",
    "valid_set = nerDataset(valid_token_seq, valid_label_seq, token_to_id, label_to_id, preprocess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nerCollator:\n",
    "\n",
    "    def __init__(self, token_padding_value, label_padding_value, percentile = 100):\n",
    "        self.token_padding_value = token_padding_value\n",
    "        self.label_padding_value = label_padding_value\n",
    "        self.percentile = percentile\n",
    "\n",
    "    def __call__(self, batch):\n",
    "\n",
    "        tokens, labels, lengths = zip(*batch)\n",
    "\n",
    "        tokens = [list(i) for i in tokens]\n",
    "        labels = [list(i) for i in labels]\n",
    "\n",
    "        max_len = int(np.percentile(lengths, self.percentile))\n",
    "\n",
    "        lengths = torch.tensor(\n",
    "            np.clip(lengths, a_min=0, a_max=max_len),\n",
    "            dtype=torch.long,\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        for i in range(len(batch)):\n",
    "            tokens[i] = torch.tensor(tokens[i][:max_len], dtype=torch.long)\n",
    "            labels[i] = torch.tensor(labels[i][:max_len], dtype=torch.long)\n",
    "\n",
    "        sorted_idx = torch.argsort(lengths, descending=True)\n",
    "        print(tokens)\n",
    "        print(self.token_padding_value)\n",
    "        \n",
    "        tokens = pad_sequence(\n",
    "            tokens, padding_value=self.token_padding_value, batch_first=True\n",
    "        )[sorted_idx]\n",
    "        labels = pad_sequence(\n",
    "            labels, padding_value=self.label_padding_value, batch_first=True\n",
    "        )[sorted_idx]\n",
    "        lengths = lengths[sorted_idx]\n",
    "\n",
    "        return tokens, labels, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_coll_fn = nerCollator(0, label_to_id[\"O\"], 100)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    "    collate_fn=train_coll_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        super(Embedding, self).__init__()\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(\n",
    "            num_embeddings=num_embeddings,\n",
    "            embedding_dim=embedding_dim,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(\n",
    "    num_embeddings=len(token_to_id),\n",
    "    embedding_dim=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 46, 128])\n",
      "torch.Size([256, 40, 128])\n",
      "torch.Size([256, 50, 128])\n",
      "torch.Size([256, 48, 128])\n",
      "torch.Size([256, 50, 128])\n",
      "torch.Size([256, 51, 128])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 44, 128])\n",
      "torch.Size([256, 51, 128])\n",
      "torch.Size([256, 60, 128])\n",
      "torch.Size([256, 44, 128])\n",
      "torch.Size([256, 44, 128])\n",
      "torch.Size([256, 57, 128])\n",
      "torch.Size([256, 50, 128])\n",
      "torch.Size([256, 44, 128])\n",
      "torch.Size([256, 50, 128])\n",
      "torch.Size([256, 38, 128])\n",
      "torch.Size([256, 51, 128])\n",
      "torch.Size([256, 50, 128])\n",
      "torch.Size([256, 52, 128])\n",
      "torch.Size([256, 55, 128])\n",
      "torch.Size([256, 49, 128])\n",
      "torch.Size([256, 52, 128])\n",
      "torch.Size([256, 46, 128])\n",
      "torch.Size([256, 47, 128])\n",
      "torch.Size([256, 62, 128])\n",
      "torch.Size([256, 50, 128])\n",
      "torch.Size([256, 50, 128])\n",
      "torch.Size([256, 47, 128])\n",
      "torch.Size([256, 78, 128])\n",
      "torch.Size([256, 55, 128])\n",
      "torch.Size([256, 59, 128])\n",
      "torch.Size([256, 49, 128])\n",
      "torch.Size([256, 53, 128])\n",
      "torch.Size([256, 55, 128])\n",
      "torch.Size([256, 47, 128])\n",
      "torch.Size([256, 113, 128])\n",
      "torch.Size([256, 60, 128])\n",
      "torch.Size([256, 48, 128])\n",
      "torch.Size([256, 80, 128])\n",
      "torch.Size([256, 44, 128])\n",
      "torch.Size([256, 59, 128])\n",
      "torch.Size([256, 60, 128])\n",
      "torch.Size([256, 53, 128])\n",
      "torch.Size([256, 69, 128])\n",
      "torch.Size([256, 47, 128])\n",
      "torch.Size([256, 41, 128])\n",
      "torch.Size([256, 43, 128])\n",
      "torch.Size([256, 77, 128])\n",
      "torch.Size([256, 46, 128])\n",
      "torch.Size([256, 55, 128])\n",
      "torch.Size([256, 62, 128])\n",
      "torch.Size([256, 55, 128])\n",
      "torch.Size([256, 52, 128])\n",
      "torch.Size([256, 109, 128])\n",
      "torch.Size([256, 105, 128])\n",
      "torch.Size([256, 83, 128])\n",
      "torch.Size([256, 62, 128])\n",
      "torch.Size([256, 61, 128])\n",
      "torch.Size([93, 54, 128])\n"
     ]
    }
   ],
   "source": [
    "for a,b,c in train_loader:\n",
    "    o = embedding_layer(a)\n",
    "    print(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSequence(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        \n",
    "        self.tokenized_text = []\n",
    "        self.tags = []\n",
    "        for i in simp(range(len(df))):\n",
    "            sent = df.loc[i,\"raw_sentence\"]\n",
    "            tags = get_sent_labels_list(i, df)\n",
    "            \n",
    "            temp = tokenizer(sent, padding='max_length', max_length=512,\n",
    "                                truncation=True, return_tensors=\"pt\")\n",
    "            self.tokenized_text.append(temp)\n",
    "            self.tags.append(tags)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.tags)\n",
    "        \n",
    "    def get_batch_text(self, id):\n",
    "        return self.tokenized_text[id]\n",
    "    \n",
    "    def get_bach_tag(self, id):\n",
    "        return self.tags[id]\n",
    "        \n",
    "    def __getitem__(self, id):\n",
    "        batch_text = self.get_batch_text(id)\n",
    "        batch_tag = self.get_bach_tag(id)\n",
    "        return batch_text, batch_tag"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
