{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from poprogress import simple_progress as simp\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence,pack_padded_sequence,pad_packed_sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = pd.read_csv(\"train.csv\")\n",
    "# val_data = pd.read_csv(\"val.csv\")\n",
    "# test1_data = pd.read_csv(\"test1.csv\")\n",
    "# test2_data = pd.read_csv(\"test2.csv\")\n",
    "\n",
    "# train_len = len(train_data)\n",
    "# val_len = len(val_data)\n",
    "# test1_len = len(test1_data)\n",
    "# test2_len = len(test2_data)\n",
    "\n",
    "# print(\"train_len: \",train_len)\n",
    "# print(\"val_len: \",val_len)\n",
    "# print(\"test1_len: \",test1_len)\n",
    "# print(\"test2_len: \",test2_len)\n",
    "# train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_len:  21363\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_sentence</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EU rejects German call to boycott British lamb .</td>\n",
       "      <td>['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MIS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Peter Blackburn</td>\n",
       "      <td>['B-PER', 'I-PER']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BRUSSELS 1996-08-22</td>\n",
       "      <td>['B-LOC', 'O']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The European Commission said on Thursday it di...</td>\n",
       "      <td>['O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany 's representative to the European Unio...</td>\n",
       "      <td>['B-LOC', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        raw_sentence  \\\n",
       "0   EU rejects German call to boycott British lamb .   \n",
       "1                                    Peter Blackburn   \n",
       "2                                BRUSSELS 1996-08-22   \n",
       "3  The European Commission said on Thursday it di...   \n",
       "4  Germany 's representative to the European Unio...   \n",
       "\n",
       "                                              labels  \n",
       "0  ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MIS...  \n",
       "1                                 ['B-PER', 'I-PER']  \n",
       "2                                     ['B-LOC', 'O']  \n",
       "3  ['O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O...  \n",
       "4  ['B-LOC', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG'...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = pd.read_csv(\"all-data.csv\")\n",
    "all_len = len(all_data)\n",
    "print(\"all_len: \",all_len)\n",
    "all_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data, train_ratio, valid_ratio):\n",
    "\n",
    "    pool = np.random.rand(len(data)) \n",
    "    mask1 = pool < train_ratio\n",
    "    offset = train_ratio + valid_ratio\n",
    "    mask2 = (pool >= train_ratio) * (pool < offset)\n",
    "    train = data[mask1].reset_index(drop=True)\n",
    "    valid = data[mask2].reset_index(drop=True)\n",
    "    test = data[~(mask1 + mask2)].reset_index(drop=True)\n",
    "    \n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_size:  14977\n",
      "valid_data_size:  3228\n",
      "test_data_size:  3158\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, test_data = split_dataset(all_data, 0.7, 0.15)\n",
    "print(\"train_data_size: \",len(train_data))\n",
    "print(\"valid_data_size: \",len(valid_data))\n",
    "print(\"test_data_size: \",len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14977/14977 [00:00<00:00, 26261.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-LOC': 0, 'B-MISC': 1, 'B-ORG': 2, 'B-PER': 3, 'I-LOC': 4, 'I-MISC': 5, 'I-ORG': 6, 'I-PER': 7, 'O': 8}\n",
      "{0: 'B-LOC', 1: 'B-MISC', 2: 'B-ORG', 3: 'B-PER', 4: 'I-LOC', 5: 'I-MISC', 6: 'I-ORG', 7: 'I-PER', 8: 'O'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_label_unique(data):\n",
    "    unique_label_list = []\n",
    "    for label in simp(data[\"labels\"]):\n",
    "        labels =  label.replace('[','').replace(']','').split(',')\n",
    "        for x in labels:\n",
    "            tag = x.replace(\"'\",'').replace(' ','')\n",
    "            if tag not in unique_label_list:\n",
    "                unique_label_list.append(tag)\n",
    "    return unique_label_list\n",
    "\n",
    "label_unique = sorted(get_label_unique(train_data))\n",
    "\n",
    "label_to_id = {k: v for v,k in enumerate(label_unique)}\n",
    "id_to_label = {k: v for k,v in enumerate(label_unique)}\n",
    "print(label_to_id)\n",
    "print(id_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_labels(data, id):\n",
    "    \n",
    "    def get_sent_labels_list(data, id):\n",
    "        labels_list = []\n",
    "        label = data.loc[id, \"labels\"]\n",
    "        labels =  label.replace('[','').replace(']','').split(',')\n",
    "        for x in labels:\n",
    "            tag = x.replace(\"'\",'').replace(' ','')\n",
    "            labels_list.append(tag)\n",
    "        return labels_list\n",
    "    \n",
    "    def get_sent_tokens_list(data, id):\n",
    "        tokens_list = []\n",
    "        tokens = data.loc[id, \"raw_sentence\"].split()\n",
    "        for token in tokens:\n",
    "            tokens_list.append(token.lower())\n",
    "        return tokens_list\n",
    "\n",
    "    tokens_list = get_sent_tokens_list(data, id)\n",
    "    labels_list = get_sent_labels_list(data, id)\n",
    "    return tokens_list, labels_list\n",
    "\n",
    "# get_tokens_labels(all_data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_seq(data):\n",
    "    data_token_seq, data_label_seq = [], []\n",
    "    for i in range(len(data)):\n",
    "        a, b = get_tokens_labels(data, i)\n",
    "        data_token_seq.append(a)\n",
    "        data_label_seq.append(b)\n",
    "    return data_token_seq, data_label_seq\n",
    "\n",
    "train_token_seq, train_label_seq = get_data_seq(train_data)\n",
    "\n",
    "token2cnt = Counter([token for sentence in train_token_seq for token in sentence])\n",
    "label_set = sorted(set(label for sentence in train_label_seq for label in sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token2id(token2cnt, min_count = 1,add_pad = True, add_unk = True):\n",
    "    '''\n",
    "    Get mapping from tokens to indices to use with Embedding layer.\n",
    "    \n",
    "    param:\n",
    "        - min_count : Do not mark number if number of words less then this value.\n",
    "    '''\n",
    "    token_to_id = {}\n",
    "\n",
    "    if add_pad:\n",
    "        token_to_id[\"<PAD>\"] = len(token_to_id)\n",
    "    if add_unk:\n",
    "        token_to_id[\"<UNK>\"] = len(token_to_id)\n",
    "\n",
    "    for token, cnt in token2cnt.items():\n",
    "        if cnt >= min_count:\n",
    "            token_to_id[token] = len(token_to_id)\n",
    "\n",
    "    return token_to_id\n",
    "token_to_id = get_token2id(token2cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nerDataset\n",
    "def process_tokens(tokens, token2id, unk: str = \"<UNK>\"):\n",
    "    return [token2id.get(token, token2id[unk]) for token in tokens]\n",
    "\n",
    "def process_labels(labels,label2id):\n",
    "    return [label2id[label] for label in labels]\n",
    "\n",
    "class nerDataset(Dataset):\n",
    "\n",
    "    def __init__(self, token_seq, label_seq, token2id, label2id, preprocess:bool = True):\n",
    "        self.token2id = token2id\n",
    "        self.label2id = label2id\n",
    "        self.preprocess = preprocess\n",
    "        \n",
    "        if preprocess:\n",
    "            self.token_seq = [process_tokens(tokens, token2id) for tokens in token_seq]\n",
    "            self.label_seq = [process_labels(labels, label2id) for labels in label_seq]\n",
    "        else:\n",
    "            self.token_seq = token_seq \n",
    "            self.label_seq = label_seq  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_seq)\n",
    "\n",
    "    def __getitem__(self, id):\n",
    "        if self.preprocess:\n",
    "            tokens = self.token_seq[id]\n",
    "            labels = self.label_seq[id]\n",
    "        else:\n",
    "            tokens = process_tokens(self.token_seq[id], self.token2id) \n",
    "            labels = process_labels(self.label_seq[id], self.label2id) \n",
    "\n",
    "        lengths = [len(tokens)]\n",
    "\n",
    "        return np.array(tokens), np.array(labels), np.array(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = nerDataset(train_token_seq, train_label_seq, token_to_id, label_to_id, preprocess=True)\n",
    "\n",
    "valid_token_seq, valid_label_seq = get_data_seq(valid_data)\n",
    "valid_set = nerDataset(valid_token_seq, valid_label_seq, token_to_id, label_to_id, preprocess=True)\n",
    "\n",
    "test_token_seq, test_label_seq = get_data_seq(test_data)\n",
    "test_set = nerDataset(test_token_seq, test_label_seq, token_to_id, label_to_id, preprocess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nerCollator:\n",
    "\n",
    "    def __init__(self, token_padding_value, label_padding_value, percentile = 100):\n",
    "        self.token_padding_value = token_padding_value\n",
    "        self.label_padding_value = label_padding_value\n",
    "        self.percentile = percentile\n",
    "\n",
    "    def __call__(self, batch):\n",
    "\n",
    "        tokens, labels, lengths = zip(*batch)\n",
    "\n",
    "        tokens = [list(i) for i in tokens]\n",
    "        labels = [list(i) for i in labels]\n",
    "        # 避免句子过长, 应该给个固定长度，暂时不给\n",
    "        max_len = int(np.percentile(lengths, self.percentile))\n",
    "\n",
    "        lengths = torch.tensor(np.clip(lengths, a_min=0, a_max=max_len), dtype=torch.long).squeeze(-1)\n",
    "\n",
    "        for i in range(len(batch)):\n",
    "            tokens[i] = torch.tensor(tokens[i][:max_len], dtype=torch.long)\n",
    "            labels[i] = torch.tensor(labels[i][:max_len], dtype=torch.long)\n",
    "\n",
    "        sorted_idx = torch.argsort(lengths, descending=True)\n",
    "        # 打补丁\n",
    "        tokens = pad_sequence(tokens, padding_value=self.token_padding_value, batch_first=True)[sorted_idx]\n",
    "        labels = pad_sequence(labels, padding_value=self.label_padding_value, batch_first=True)[sorted_idx]\n",
    "        lengths = lengths[sorted_idx]\n",
    "\n",
    "        return tokens, labels, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_coll_fn = nerCollator(token_to_id[\"<UNK>\"], label_to_id[\"O\"], 100)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    "    collate_fn=train_coll_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        super(Embedding, self).__init__()\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(num_embeddings=len(token_to_id), embedding_dim=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicRNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        rnn_unit: torch.nn.Module,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        num_layers: int,\n",
    "        dropout: float,\n",
    "        bidirectional: bool,\n",
    "    ):\n",
    "        super(DynamicRNN, self).__init__()\n",
    "        self.rnn = rnn_unit(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, x_length):\n",
    "        packed_x = pack_padded_sequence(x, x_length.cpu(), batch_first=True, enforce_sorted=True)\n",
    "        packed_rnn_out, _ = self.rnn(packed_x)\n",
    "        rnn_out, _ = pad_packed_sequence(packed_rnn_out, batch_first=True)\n",
    "        return rnn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_layer = DynamicRNN(\n",
    "    rnn_unit=torch.nn.LSTM,\n",
    "    input_size=128,  # ref to emb_dim\n",
    "    hidden_size=256,\n",
    "    num_layers=1,\n",
    "    dropout=0,\n",
    "    bidirectional=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearHead(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Linear layer wrapper.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, linear_head):\n",
    "        super(LinearHead, self).__init__()\n",
    "        self.linear_head = linear_head\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_head = LinearHead(\n",
    "    linear_head=torch.nn.Linear(\n",
    "        in_features=(2*256),\n",
    "        out_features=len(label_to_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_layer, rnn_layer, linear_head):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.embedding = embedding_layer  \n",
    "        self.rnn = rnn_layer \n",
    "        self.linear_head = linear_head  \n",
    "\n",
    "    def forward(self, x, x_length):\n",
    "        embed = self.embedding(x) \n",
    "        rnn_out = self.rnn(embed, x_length) \n",
    "        logits = self.linear_head(rnn_out)  \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM(\n",
    "    embedding_layer=embedding_layer,\n",
    "    rnn_layer=rnn_layer,\n",
    "    linear_head=linear_head,\n",
    ")#.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss(reduction=\"none\")  # hardcoded\n",
    "\n",
    "optimizer_type = torch.optim.Adam\n",
    "optimizer = optimizer_type(params=model.parameters(), lr=0.001, amsgrad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masking(lengths: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.arange(end=lengths.max(), device=lengths.device).expand(size=(lengths.shape[0], lengths.max())) < lengths.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(tensor: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert torch.Tensor to np.ndarray.\n",
    "    \"\"\"\n",
    "    return (tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def calculate_metrics(\n",
    "    metrics,\n",
    "    loss: float,\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    idx2label,\n",
    "):# -> DefaultDict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    Calculate metrics on epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    metrics[\"loss\"].append(loss)\n",
    "\n",
    "    f1_per_class = f1_score(y_true=y_true, y_pred=y_pred, labels=range(len(idx2label)), average=None)\n",
    "    f1_weighted = f1_score(y_true=y_true, y_pred=y_pred, average=\"weighted\")\n",
    "    for cls, f1 in enumerate(f1_per_class):\n",
    "        metrics[f\"f1 {idx2label[cls]}\"].append(f1)\n",
    "    metrics[\"f1-weighted\"].append(f1_weighted)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model,\n",
    "    dataloader: DataLoader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    device: torch.device,\n",
    "    clip_grad_norm: float,\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Training loop on one epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    metrics = defaultdict(list)\n",
    "    # idx2label = {v: k for k, v in dataloader.dataset.label2idx.items()}\n",
    "\n",
    "    if verbose:\n",
    "        dataloader = tqdm(dataloader)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for tokens, labels, lengths in simp(dataloader):\n",
    "        tokens, labels, lengths = (\n",
    "            tokens.to(device),\n",
    "            labels.to(device),\n",
    "            lengths.to(device),\n",
    "        )\n",
    "\n",
    "        mask = masking(lengths)\n",
    "\n",
    "        # forward pass\n",
    "        logits = model(tokens, lengths)\n",
    "        loss_without_reduction = criterion(logits.transpose(-1, -2), labels)\n",
    "        loss = torch.sum(loss_without_reduction * mask) / torch.sum(mask)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(),\n",
    "            max_norm=clip_grad_norm,\n",
    "            norm_type=2,\n",
    "        )\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # make predictions\n",
    "        y_true = to_numpy(labels[mask])\n",
    "        y_pred = to_numpy(logits.argmax(dim=-1)[mask])\n",
    "\n",
    "        # calculate metrics\n",
    "        metrics = calculate_metrics(\n",
    "            metrics=metrics,\n",
    "            loss=loss.item(),\n",
    "            y_true=y_true,\n",
    "            y_pred=y_pred,\n",
    "            idx2label=id_to_label,\n",
    "        )\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(\n",
    "    model,\n",
    "    dataloader: DataLoader,\n",
    "    criterion,\n",
    "    device: torch.device,\n",
    "    verbose: bool = True,\n",
    "):# -> DefaultDict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    Validate loop on one epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    metrics = defaultdict(list)\n",
    "    # idx2label = {v: k for k, v in dataloader.dataset.label2idx.items()}\n",
    "\n",
    "    if verbose:\n",
    "        dataloader = tqdm(dataloader)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for tokens, labels, lengths in dataloader:\n",
    "        tokens, labels, lengths = (\n",
    "            tokens.to(device),\n",
    "            labels.to(device),\n",
    "            lengths.to(device),\n",
    "        )\n",
    "\n",
    "        mask = masking(lengths)\n",
    "\n",
    "        # forward pass\n",
    "        with torch.no_grad():\n",
    "            logits = model(tokens, lengths)\n",
    "            loss_without_reduction = criterion(logits.transpose(-1, -2), labels)\n",
    "            loss = torch.sum(loss_without_reduction * mask) / torch.sum(mask)\n",
    "\n",
    "        # make predictions\n",
    "        y_true = to_numpy(labels[mask])\n",
    "        y_pred = to_numpy(logits.argmax(dim=-1)[mask])\n",
    "\n",
    "        # calculate metrics\n",
    "        metrics = calculate_metrics(\n",
    "            metrics=metrics,\n",
    "            loss=loss.item(),\n",
    "            y_true=y_true,\n",
    "            y_pred=y_pred,\n",
    "            idx2label=id_to_label,\n",
    "        )\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(\n",
    "    model,\n",
    "    train_loader: DataLoader,\n",
    "    valid_loader: DataLoader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    device: torch.device,\n",
    "    clip_grad_norm,\n",
    "    n_epoch,\n",
    "    tensorboard,\n",
    "    logger,\n",
    "    verbose,\n",
    "    test_loader):\n",
    "    \n",
    "    for epoch in tqdm(range(n_epoch)):\n",
    "\n",
    "        if verbose:\n",
    "            logger.info(f\"epoch [{epoch+1}/{n_epoch}]\\n\")\n",
    "\n",
    "        train_metrics = train_epoch(\n",
    "            model=model,\n",
    "            dataloader=train_loader,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "            clip_grad_norm=clip_grad_norm,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        if verbose:\n",
    "            for metric_name, metric_list in train_metrics.items():\n",
    "                logger.info(f\"train {metric_name}: {np.mean(metric_list)}\")\n",
    "            logger.info(\"\\n\")\n",
    "\n",
    "        valid_metrics = validate_epoch(\n",
    "            model=model,\n",
    "            dataloader=valid_loader,\n",
    "            criterion=criterion,\n",
    "            device=device,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        if verbose:\n",
    "            for metric_name, metric_list in valid_metrics.items():\n",
    "                logger.info(f\"valid {metric_name}: {np.mean(metric_list)}\")\n",
    "            logger.info(\"\\n\")\n",
    "\n",
    "    if test_loader is not None:\n",
    "        # if tensorboard:\n",
    "        #     writer.add_scalar(\"Loss/train\", np.mean((train_metrics[\"loss\"])), epoch)\n",
    "        #     writer.add_scalar(\"Loss/val\", np.mean((valid_metrics[\"loss\"])), epoch)\n",
    "\n",
    "        test_metrics = validate_epoch(\n",
    "            model=model,\n",
    "            dataloader=test_loader,\n",
    "            criterion=criterion,\n",
    "            device=device,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        if verbose:\n",
    "            for metric_name, metric_list in test_metrics.items():\n",
    "                logger.info(f\"test {metric_name}: {np.mean(metric_list)}\")\n",
    "            logger.info(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_coll_fn = nerCollator(token_to_id[\"<UNK>\"], label_to_id[\"O\"], 100)\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    dataset=valid_set,\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    "    collate_fn=valid_coll_fn,\n",
    ")\n",
    "test_coll_fn = nerCollator(token_to_id[\"<UNK>\"], label_to_id[\"O\"], 100)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_set,\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    "    collate_fn=test_coll_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'info'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[441], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclip_grad_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensorboard\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[434], line 18\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(model, train_loader, valid_loader, criterion, optimizer, device, clip_grad_norm, n_epoch, tensorboard, logger, verbose, test_loader)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(n_epoch)):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[0;32m---> 18\u001b[0m         \u001b[43mlogger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m     train_metrics \u001b[38;5;241m=\u001b[39m train_epoch(\n\u001b[1;32m     21\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     22\u001b[0m         dataloader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m     28\u001b[0m     )\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'bool' object has no attribute 'info'"
     ]
    }
   ],
   "source": [
    "train_loop(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        valid_loader=valid_loader,\n",
    "        test_loader=test_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        clip_grad_norm=0.1,\n",
    "        n_epoch=2, \n",
    "        logger=False,\n",
    "        verbose=False,\n",
    "        tensorboard=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 120, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 130, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'nerDataset' on <module '__main__' (built-in)>\n",
      "  0%|          | 0/59 [00:18<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m metrics \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tokens, labels, lengths \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader):\n\u001b[1;32m     10\u001b[0m     tokens, labels, lengths \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     11\u001b[0m         tokens\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     12\u001b[0m         labels\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     13\u001b[0m         lengths\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     14\u001b[0m     )\n\u001b[1;32m     16\u001b[0m     mask \u001b[38;5;241m=\u001b[39m masking(lengths)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:441\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:388\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1042\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1035\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1042\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/context.py:288\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(fp\u001b[38;5;241m.\u001b[39mgetbuffer())\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "verbose = True\n",
    "n_epoch = 1\n",
    "clip_grad_norm = 0.1\n",
    "for epoch in range(n_epoch):\n",
    "\n",
    "    metrics = defaultdict(list)\n",
    "    model.train()\n",
    "\n",
    "    for tokens, labels, lengths in tqdm(train_loader):\n",
    "        tokens, labels, lengths = (\n",
    "            tokens.to(device),\n",
    "            labels.to(device),\n",
    "            lengths.to(device),\n",
    "        )\n",
    "\n",
    "        mask = masking(lengths)\n",
    "\n",
    "        # forward pass\n",
    "        logits = model(tokens, lengths)\n",
    "        loss_without_reduction = criterion(logits.transpose(-1, -2), labels)\n",
    "        loss = torch.sum(loss_without_reduction * mask) / torch.sum(mask)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(),\n",
    "            max_norm=clip_grad_norm,\n",
    "            norm_type=2,\n",
    "        )\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # make predictions\n",
    "        y_true = to_numpy(labels[mask])\n",
    "        y_pred = to_numpy(logits.argmax(dim=-1)[mask])\n",
    "\n",
    "        # calculate metrics\n",
    "        metrics = calculate_metrics(\n",
    "            metrics=metrics,\n",
    "            loss=loss.item(),\n",
    "            y_true=y_true,\n",
    "            y_pred=y_pred,\n",
    "            idx2label=id_to_label,\n",
    "        )\n",
    "        \n",
    "    # train_metrics = train_epoch(\n",
    "    #     model=model,\n",
    "    #     dataloader=train_loader,\n",
    "    #     criterion=criterion,\n",
    "    #     optimizer=optimizer,\n",
    "    #     device=device,\n",
    "    #     clip_grad_norm=0.1,\n",
    "    #     verbose=verbose,\n",
    "    # )\n",
    "\n",
    "    # valid_metrics = validate_epoch(\n",
    "    #     model=model,\n",
    "    #     dataloader=valid_loader,\n",
    "    #     criterion=criterion,\n",
    "    #     device=device,\n",
    "    #     verbose=verbose,\n",
    "    # )\n",
    "\n",
    "# if test_loader is not None:\n",
    "#     # if tensorboard:\n",
    "#     #     writer.add_scalar(\"Loss/train\", np.mean((train_metrics[\"loss\"])), epoch)\n",
    "#     #     writer.add_scalar(\"Loss/val\", np.mean((valid_metrics[\"loss\"])), epoch)\n",
    "\n",
    "#     test_metrics = validate_epoch(\n",
    "#         model=model,\n",
    "#         dataloader=test_loader,\n",
    "#         criterion=criterion,\n",
    "#         device=device,\n",
    "#         verbose=verbose,\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
